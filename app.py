# -*- coding: utf-8 -*-
import os
import sys
import datetime
import gradio as gr
import numpy as np
import random
# import spaces #[uncomment to use ZeroGPU]
import torch
from torchvision.transforms import ToTensor, ToPILImage

base_path = '/home/xlab-app-center/UltraFusionModel'
# download repo to the base_path directory using git
print(os.system('pwd'))
auth_token = os.getenv("ModelAccessToken")
# please replace "your_git_token" with real token
os.system(f'git clone https://OpenImagingLab:{auth_token}@code.openxlab.org.cn/OpenImagingLab/UltraFusionModel.git {base_path}')
os.system(f'cd {base_path} && git lfs pull')
os.system(f'mv {base_path}/* ./')
print(os.system('pwd'))
print(os.system('ls'))

from ultrafusion_utils import load_model, run_ultrafusion, check_input

RUN_TIMES = 0

to_tensor = ToTensor()
to_pil = ToPILImage()
ultrafusion_pipe, flow_model = load_model()

device = "cuda" if torch.cuda.is_available() else "cpu"
if torch.cuda.is_available():
    torch_dtype = torch.float16
else:
    torch_dtype = torch.float32

MAX_SEED = np.iinfo(np.int32).max
MAX_IMAGE_SIZE = 1024

# @spaces.GPU(duration=60) #[uncomment to use ZeroGPU]
def infer(
    under_expo_img,
    over_expo_img,
    num_inference_steps
):
    print(under_expo_img.size)
    print("reciving image")
    
    under_expo_img_lr, over_expo_img_lr, under_expo_img, over_expo_img, use_bgu = check_input(under_expo_img, over_expo_img, max_l=1500)

    ue = to_tensor(under_expo_img_lr).unsqueeze(dim=0).to("cuda")
    oe = to_tensor(over_expo_img_lr).unsqueeze(dim=0).to("cuda")
    ue_hr = to_tensor(under_expo_img).unsqueeze(dim=0).to("cuda")
    oe_hr = to_tensor(over_expo_img).unsqueeze(dim=0).to("cuda")

    print("num_inference_steps:", num_inference_steps)
    try:
        if num_inference_steps is None:
            num_inference_steps = 20
        num_inference_steps = int(num_inference_steps)
    except Exception as e:
        num_inference_steps = 20

    out = run_ultrafusion(ue, oe, ue_hr, oe_hr, use_bgu, 'test', flow_model=flow_model, pipe=ultrafusion_pipe, steps=num_inference_steps, consistent_start=None)
    
    out = out.clamp(0, 1).squeeze()
    out_pil = to_pil(out)

    global RUN_TIMES
    RUN_TIMES = RUN_TIMES + 1
    print("---------------------------- Using Times---------------------------------------")
    print(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Using times: {RUN_TIMES}")

    return out_pil


def build_demo():
    examples= [
        [os.path.join("examples", img_name, "ue.jpg"), 
        os.path.join("examples", img_name, "oe.jpg")] for img_name in sorted(os.listdir("examples"))
    ]
    IMG_W = 320
    IMG_H = 240
    css = """
    #col-container {
        margin: 0 auto;
        max-width: 640px;
    }
    """
    # max-heigh: 1500px;

    _README_ = r"""

    - è¿™æ˜¯ä¸€ä¸ªå¯ä»¥èåˆä¸¤å¼ ä¸åŒæ›å…‰å›¾åƒçš„ HDR ç®—æ³•ã€‚

    - å®ƒèƒ½å¤Ÿèåˆæ›å…‰å·®å¼‚éå¸¸å¤§çš„ä¸¤å¼ å›¾åƒï¼Œç”šè‡³æ›å…‰å·®å¼‚é«˜è¾¾9æ¡£ã€‚
    
    - ä¸¤å¼ è¾“å…¥å›¾åƒåº”å…·æœ‰ç›¸åŒçš„åˆ†è¾¨ç‡ï¼›å¦åˆ™ï¼Œå°†å¾—åˆ°å¼‚å¸¸çš„ç»“æœã€‚

    - æˆ‘ä»¬ä¸ä¼šå­˜å‚¨æ‚¨ä¸Šä¼ çš„ä»»ä½•æ•°æ®åŠå…¶å¤„ç†ç»“æœã€‚

    """
    # - The maximum resolution we support is 1500 x 1500. If the images you upload are larger than this, they will be downscaled while maintaining the original aspect ratio.
    # - This is only for internal testing. Do not share it publicly.
    _CITE_ = r"""
    ğŸ“ **Citation**

    If you find our work useful for your research or applications, please cite using this bibtex:
    ```bibtex
    @article{xxx,
    title={xxx},
    author={xxx},
    journal={arXiv preprint arXiv:xx.xx},
    year={2024}
    }
    ```

    ğŸ“‹ **License**

    CC BY-NC 4.0. LICENSE.

    ğŸ“§ **Contact**

    If you have any questions, feel free to open a discussion or contact us at <b>xxx@gmail.com</b>.
    """

    with gr.Blocks(css=css) as demo:
        with gr.Column(elem_id="col-container"):
            gr.Markdown("""<h1 style="text-align: center; font-size: 32px;"><b>æµ¦åƒâ€¢è¶…çº§HDRğŸ“¸âœ¨</b></h1>""")
            gr.Markdown("""<h1 style="text-align: center; font-size: 24px;"><b>è¯¥å¦‚ä½•ä½¿ç”¨å®ƒå‘¢ï¼Ÿ</b></h1>""")
            with gr.Row():
                gr.Image("ui/ch-short.png", width=IMG_W//3, show_label=False, interactive=False, show_download_button=False)
                gr.Image("ui/ch-long.png", width=IMG_W//3, show_label=False, interactive=False, show_download_button=False)
                gr.Image("ui/ch-run.png", width=IMG_W//3, show_label=False, interactive=False, show_download_button=False)
            
            with gr.Row():
                gr.Markdown("""<h1 style="text-align: center; font-size: 12px;"><b>â€ ç‚¹å‡»æ‹ç…§ç•Œé¢ï¼Œå‘ä¸‹æ‹–åŠ¨â˜€ï¸å›¾æ ‡ï¼Œæ‹æ‘„çŸ­æ›å…‰ç…§ç‰‡ã€‚</b></h1>""")
                gr.Markdown("""<h1 style="text-align: center; font-size: 12px;"><b>â ç‚¹å‡»æ‹ç…§ç•Œé¢ï¼Œå‘ä¸Šæ‹–åŠ¨â˜€ï¸å›¾æ ‡ï¼Œæ‹æ‘„é•¿æ›å…‰ç…§ç‰‡ã€‚</b></h1>""")
                gr.Markdown("""<h1 style="text-align: center; font-size: 12px;"><b>â‚ ä¸Šä¼ æ‹æ‘„çš„çŸ­æ›å…‰å’Œé•¿æ›å…‰ç…§ç‰‡ï¼Œéšåç‚¹å‡»â€œè¿è¡Œâ€æŒ‰é’®ï¼Œè·å–å¤„ç†åçš„ç»“æœã€‚</b></h1>""")

            gr.Markdown("""<h1 style="text-align: center; font-size: 24px;"><b>å¼€å§‹ä½“éªŒå®ƒå§!</b></h1>""")
            with gr.Row():
                under_expo_img = gr.Image(label="çŸ­çˆ†å…‰å›¾", show_label=True,
                    image_mode="RGB",
                    sources=["upload", ],
                    width=IMG_W,
                    height=IMG_H,
                    type="pil"
                )
                over_expo_img = gr.Image(label="é•¿æ›å…‰å›¾", show_label=True, 
                    image_mode="RGB",
                    sources=["upload", ],
                    width=IMG_W,
                    height=IMG_H,
                    type="pil"
                )
            with gr.Row():
                run_button = gr.Button("è¿è¡Œ", variant="primary") # scale=0, 
            
            result = gr.Image(label="ç»“æœ", show_label=True, 
                            type='pil', 
                            image_mode='RGB', 
                            format="png",
                            width=IMG_W*2,
                            height=IMG_H*2,
                            )
            gr.Markdown(r"""<h1 style="text-align: center; font-size: 18px;"><b>å–œæ¬¢ä¸Šé¢çš„ç»“æœå—ï¼Ÿç‚¹å‡»å›¾åƒä¸Šçš„ä¸‹è½½æŒ‰é’®ğŸ“¥å³å¯ä¸‹è½½ã€‚</b></h1>""") # width="100" height="100"  <img src="ui/download.svg" alt="download"> 
            with gr.Accordion("é«˜çº§è®¾ç½®", open=True):
                num_inference_steps = gr.Slider(
                    label="æ‰©æ•£æ¨¡å‹çš„æ¨ç†æ­¥é•¿",
                    minimum=2,
                    maximum=50,
                    step=1,
                    value=20,  # Replace with defaults that work for your model
                    interactive=True
                )
        
            gr.Examples(
                examples=examples, 
                inputs=[under_expo_img, over_expo_img, num_inference_steps], 
                label="ç¤ºä¾‹",
                # examples_per_page=10,
                fn=infer,
                cache_examples=True,
                outputs=[result,],
                )
            gr.Markdown(_README_)
            # gr.Markdown(_CITE_)
        run_button.click(fn=infer,
                        inputs=[under_expo_img, over_expo_img, num_inference_steps],
                        outputs=[result,],
                        )
    return demo

if __name__ == "__main__":
    demo = build_demo()
    demo.queue(max_size=10)
    demo.launch(share=True)
    # demo.launch(server_name="0.0.0.0", debug=True, show_api=True, show_error=True, share=False)
